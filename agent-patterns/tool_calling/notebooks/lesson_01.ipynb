{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "\n",
    "Tool calling is a pattern where an agent uses a tool to perform a task that enables it to take actions in the real world. Examples include:\n",
    "\n",
    "- An agent using a tool to browse the web to find information.\n",
    "- An agent using a tool to place an order on a website.\n",
    "- An agent using a tool to send emails.\n",
    "\n",
    "\n",
    "###Why Is This Important?\n",
    "\n",
    "Large Language Models (LLMs) are not able to take actions in the real world. They can only generate text that they have been trained on. Sometimes, this information may be outdated or incorrect or not custom to your application. In such a case, you want to use a tool to retrieve most relevant information and use it to generate a response. Or you want to use a tool to take an action in the real world.\n",
    "\n",
    "In cases you want to use the LLM to perfect what it can generated, in the last lesson, we saw how to use the Reflection agent to improve the LLM's output. Sometimes, you want the LLM to take an action in the real world. For example, you want to use the LLM to send an email to a client. In such a case, you want to use a tool to send an email.\n",
    "\n",
    "\n",
    "### How To Use Tool Calling\n",
    "\n",
    "To achieve tool calling, we simply write a Python function, let's say a function to add two numbers. We then pass this function to the LLM as a tool. The LLM can then use this tool to add two numbers.\n",
    "\n",
    "Okay, so how do we do this?\n",
    "\n",
    "\n",
    "#### Step 1: Write a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by writing a function to add two numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: When writing this functions, make sure to use docstrings to describe the function and type hints to describe the parameters and return type. This will help the LLM understand the function and use it correctly.\n",
    "\n",
    "\n",
    "### Step 2: Write LLM Prompt Description The Function And How To Use It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_calling_prompt = \"\"\"You are an AI assistant with function calling capabilities. Your primary role is to interpret user requests and call appropriate functions when needed.\n",
    "\n",
    "When presented with function definitions within <tools></tools> XML tags, you should:\n",
    "\n",
    "1. Analyze the user's request to determine if a function call is necessary\n",
    "2. Carefully inspect the function signature, paying close attention to parameter types and requirements\n",
    "3. When calling a function, format your response using the following structure:\n",
    "\n",
    "<tool_call>\n",
    "{\"name\": \"function_name\", \"arguments\": {\"param1\": \"value1\", \"param2\": \"value2\"}}\n",
    "</tool_call>\n",
    "\n",
    "Important guidelines:\n",
    "- Never make assumptions about parameter values - use only information explicitly provided by the user\n",
    "- Respect parameter types as specified in the function definitions (e.g., string, integer, boolean)\n",
    "- You may call multiple functions if necessary to fulfill the request\n",
    "- If missing critical information needed for a function call, ask the user for clarification\n",
    "\n",
    "Function definitions will be provided in this format:\n",
    "<tools>\n",
    "{\n",
    "    \"name\": \"function_name\",\n",
    "    \"description\": \"Function description\",\n",
    "    \"parameters\": {\n",
    "        \"properties\": {\n",
    "            \"parameter1\": {\n",
    "                \"type\": \"data_type\"\n",
    "            },\n",
    "            \"parameter2\": {\n",
    "                \"type\": \"data_type\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "</tools>\n",
    "\n",
    "Ensure your function calls use valid JSON syntax and include all required parameters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this prompt, the LLM will understand and return an output similar to this XML format:\n",
    "\n",
    "```json\n",
    "<tool_call>\n",
    "{\"name\": \"add_two_numbers\", \"arguments\": {\"a\": 1, \"b\": 2}}\n",
    "</tool_call>\n",
    "```\n",
    "\n",
    "Now, let's write a prompt to the LLM to use the function we wrote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for the LLM based on the function_calling_prompt for the add_two_numbers function\n",
    "system_prompt = \"\"\"You are an AI assistant with function calling capabilities. Your primary role is to interpret user requests and call appropriate functions when needed.\n",
    "\n",
    "When presented with function definitions within <tools></tools> XML tags, you should:\n",
    "\n",
    "1. Analyze the user's request to determine if a function call is necessary\n",
    "2. Carefully inspect the function signature, paying close attention to parameter types and requirements\n",
    "3. When calling a function, format your response using the following structure:\n",
    "\n",
    "<tool_call>\n",
    "{\n",
    "    \"name\": \"add_two_numbers\",\n",
    "    \"description\": \"Used to add two numbers together\",\n",
    "    \"parameters\": {\n",
    "        \"properties\": {\n",
    "            \"a\": {\n",
    "                \"type\": \"int\"\n",
    "            },\n",
    "            \"b\": {\n",
    "                \"type\": \"int\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "\n",
    "<tool_call>\n",
    "{\"name\": \"add_two_numbers\", \"arguments\": {\"a\": 1, \"b\": 2}}\n",
    "</tool_call>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the system prompt, we can use it to call the LLM.\n",
    "\n",
    "NOTE: Use an LLM that supports function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_call_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the sum of 1 and 2. Use the tool provided to you.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=function_call_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the sum of 1 and 2, I'll use a function call.\n",
      "\n",
      "<tool_call>\n",
      "{\"name\": \"add_two_numbers\", \"arguments\": {\"a\": 1, \"b\": 2}}\n",
      "</tool_call>\n"
     ]
    }
   ],
   "source": [
    "response_content = response.choices[0].message.content\n",
    "print(response_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the query input, I explicitly asked the LLM to use the tool provided to it. This is because the LLM can naturally add the two numbers together without using the tool. I need to explicitly tell it to use the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_response_to_dict(response_content: str) -> dict:\n",
    "    try:\n",
    "        match = re.search(r\"<tool_call>\\s*(\\{.*?\\})\\s*</tool_call>\", response_content, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(\"No <tool_call> JSON found in response.\")\n",
    "        json_str = match.group(1)\n",
    "        return json.loads(json_str)\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        return {\n",
    "            \"name\": type(e).__name__,\n",
    "            \"message\": str(e),\n",
    "            \"stack\": None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'add_two_numbers', 'arguments': {'a': 1, 'b': 2}}\n"
     ]
    }
   ],
   "source": [
    "transformed_response = transform_response_to_dict(response_content)\n",
    "print(transformed_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "function_call_response = add_two_numbers(**transformed_response[\"arguments\"])\n",
    "print(function_call_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this results from the function call, we can pass the output from here to the LLM in a prompt as an observation and the a more natural language response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems like you want to add two numbers using a function tool. I will do that for you.\\n\\n<tool_call>\\n{\"name\": \"add_two_numbers\", \"arguments\": {\"a\": 1, \"b\": 2}}\\n</tool_call>'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_call_prompt.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Observation from tool call: {function_call_response}\"\n",
    "})\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=function_call_prompt,\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Tool Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Any\n",
    "\n",
    "def extract_function_metadata(function: Callable) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Creates a metadata dictionary from a function's signature.\n",
    "    \n",
    "    Parameters:\n",
    "        function: The target function to analyze\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the function's name, documentation, and parameter specifications\n",
    "    \"\"\"\n",
    "    # Initialize the basic structure\n",
    "    metadata = {\n",
    "        \"name\": function.__name__,\n",
    "        \"description\": function.__doc__,\n",
    "        \"parameters\": {\"properties\": {}}\n",
    "    }\n",
    "    \n",
    "    # Extract parameter types (excluding return annotation)\n",
    "    parameter_types = {\n",
    "        param_name: {\"type\": param_type.__name__}\n",
    "        for param_name, param_type in function.__annotations__.items()\n",
    "        if param_name != \"return\"\n",
    "    }\n",
    "    \n",
    "    # Add parameters to metadata\n",
    "    metadata[\"parameters\"][\"properties\"] = parameter_types\n",
    "    return metadata\n",
    "\n",
    "def convert_argument_types(tool_invocation: Dict[str, Any], function_spec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ensures all arguments match their expected types according to the function specification.\n",
    "    \n",
    "    Parameters:\n",
    "        tool_invocation: Dictionary with the tool name and arguments\n",
    "        function_spec: Dictionary containing the expected parameter types\n",
    "        \n",
    "    Returns:\n",
    "        Updated tool invocation with correctly typed arguments\n",
    "    \"\"\"\n",
    "    expected_params = function_spec[\"parameters\"][\"properties\"]\n",
    "    \n",
    "    # Type conversion mapping\n",
    "    type_converters = {\n",
    "        \"int\": int,\n",
    "        \"str\": str,\n",
    "        \"bool\": bool,\n",
    "        \"float\": float\n",
    "    }\n",
    "    \n",
    "    # Convert each argument to its expected type if needed\n",
    "    for arg_name, arg_value in tool_invocation[\"arguments\"].items():\n",
    "        target_type = expected_params[arg_name].get(\"type\")\n",
    "        if not isinstance(arg_value, type_converters[target_type]):\n",
    "            tool_invocation[\"arguments\"][arg_name] = type_converters[target_type](arg_value)\n",
    "            \n",
    "    return tool_invocation\n",
    "\n",
    "class Tool:\n",
    "    \"\"\"\n",
    "    Wrapper class that encapsulates a function as a callable tool.\n",
    "    \n",
    "    Attributes:\n",
    "        name: Tool identifier\n",
    "        function: The underlying function\n",
    "        specification: JSON-formatted function metadata\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, function: Callable, specification: str):\n",
    "        self.name = name\n",
    "        self.function = function\n",
    "        self.specification = specification\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.specification\n",
    "        \n",
    "    def execute(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Runs the wrapped function with the provided arguments.\n",
    "        \n",
    "        Parameters:\n",
    "            **kwargs: Arguments to pass to the function\n",
    "            \n",
    "        Returns:\n",
    "            The function's result\n",
    "        \"\"\"\n",
    "        return self.function(**kwargs)\n",
    "\n",
    "def tool(function: Callable) -> Tool:\n",
    "    \"\"\"\n",
    "    Decorator that transforms a regular function into a Tool instance.\n",
    "    \n",
    "    Parameters:\n",
    "        function: The function to convert into a tool\n",
    "        \n",
    "    Returns:\n",
    "        A fully configured Tool object\n",
    "    \"\"\"\n",
    "    def create_tool():\n",
    "        metadata = extract_function_metadata(function)\n",
    "        return Tool(\n",
    "            name=metadata.get(\"name\"),\n",
    "            function=function,\n",
    "            specification=json.dumps(metadata)\n",
    "        )\n",
    "    \n",
    "    return create_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculate_area_of_rectangle(length: float, width: float) -> float:\n",
    "    \"\"\"Calculate the area of a rectangle.\"\"\"\n",
    "    return length * width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"calculate_area_of_rectangle\", \"description\": \"Calculate the area of a rectangle.\", \"parameters\": {\"properties\": {\"length\": {\"type\": \"float\"}, \"width\": {\"type\": \"float\"}}}}'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_area_of_rectangle.specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_area_of_rectangle.execute(length=10, width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def understanding_tool_decorator():\n",
    "    # Print tool specifications\n",
    "    print(\"Available tools:\")\n",
    "    print(f\"1. {calculate_area_of_rectangle}\")\n",
    "    \n",
    "    # Example tool call (simulating AI model output)\n",
    "    area_tool_call = {\n",
    "        \"name\": \"calculate_area\",\n",
    "        \"arguments\": {\n",
    "            \"length\": \"10\",  # String instead of float to demonstrate type conversion\n",
    "            \"width\": 5.5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Parse the tool specification to get expected types\n",
    "    area_spec = json.loads(calculate_area_of_rectangle.specification)\n",
    "    \n",
    "    # Validate and convert argument types\n",
    "    validated_call = convert_argument_types(area_tool_call, area_spec)\n",
    "    \n",
    "    # Execute the tool\n",
    "    result = calculate_area_of_rectangle.execute(**validated_call[\"arguments\"])\n",
    "    print(f\"\\nCalculated area: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "1. {\"name\": \"calculate_area_of_rectangle\", \"description\": \"Calculate the area of a rectangle.\", \"parameters\": {\"properties\": {\"length\": {\"type\": \"float\"}, \"width\": {\"type\": \"float\"}}}}\n",
      "\n",
      "Calculated area: 55.0\n"
     ]
    }
   ],
   "source": [
    "understanding_tool_decorator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple explanation of how the tool decorator works:\n",
    "\n",
    "1. The `@tool` decorator is used to transform a regular function into a Tool instance.\n",
    "2. The `extract_function_metadata` function is used to extract the function's name, documentation, and parameter specifications.\n",
    "3. The `convert_argument_types` function is used to ensure all arguments match their expected types according to the function specification.\n",
    "4. The `Tool` class is used to encapsulate the function as a callable tool.\n",
    "5. The `execute` method is used to run the wrapped function with the provided arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Tool Calling Agent\n",
    "\n",
    "Now we can move on ahead and implement the tool calling agent. For this, we'll create a Python class that we can pass in tools to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Callable\n",
    "\n",
    "class ToolCallingAgent:\n",
    "    \"\"\"\n",
    "    An agent that integrates language models with function-calling tools.\n",
    "    \n",
    "    This class manages the interaction between a language model and a set of tools,\n",
    "    allowing the model to decide when to call functions and processing the results.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Callable, system_prompt: str, tools: List[Any]):\n",
    "        \"\"\"\n",
    "        Initialize an AI agent with a language model and tools.\n",
    "        \n",
    "        Args:\n",
    "            llm: Function that takes a model name and messages and returns a response\n",
    "            system_prompt: Instructions for guiding the language model's behavior\n",
    "            tools: List of Tool objects created with the @tool decorator\n",
    "        \"\"\"\n",
    "        self.model = llm\n",
    "        self.system_prompt = system_prompt\n",
    "        self.tools = {tool.name: tool for tool in tools}\n",
    "        self.conversation_history = []\n",
    "        self.max_iterations = 5  # Prevent infinite loops\n",
    "        \n",
    "    def format_tools_for_prompt(self) -> str:\n",
    "        \"\"\"Format all available tools into a format the language model can understand.\"\"\"\n",
    "        tools_json = []\n",
    "        for tool in self.tools.values():\n",
    "            try:\n",
    "                # Use the specification provided by the @tool decorator\n",
    "                tools_json.append(json.loads(tool.specification))\n",
    "            except (json.JSONDecodeError, AttributeError):\n",
    "                # Fallback for tools without proper specification\n",
    "                tools_json.append({\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": getattr(tool, \"description\", \"No description available\"),\n",
    "                    \"parameters\": {\"properties\": {}}\n",
    "                })\n",
    "                \n",
    "        return f\"<tools>\\n{json.dumps(tools_json, indent=2)}\\n</tools>\"\n",
    "    \n",
    "    def extract_tool_calls(self, response: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Parse the language model's response to extract tool call requests.\n",
    "        \n",
    "        Args:\n",
    "            response: The text response from the language model\n",
    "            \n",
    "        Returns:\n",
    "            A list of tool call dictionaries with 'name' and 'arguments' keys\n",
    "        \"\"\"\n",
    "        tool_calls = []\n",
    "        pattern = r\"<tool_call>(.*?)</tool_call>\"\n",
    "        matches = re.findall(pattern, response, re.DOTALL)\n",
    "        \n",
    "        for match in matches:\n",
    "            try:\n",
    "                tool_call = json.loads(match.strip())\n",
    "                if \"name\" in tool_call and \"arguments\" in tool_call:\n",
    "                    tool_calls.append(tool_call)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        \n",
    "        # print(tool_calls)\n",
    "        \n",
    "        return tool_calls\n",
    "    \n",
    "    def execute_tool(self, tool_call: Dict[str, Any]) -> Any:\n",
    "        \"\"\"\n",
    "        Execute a tool based on the model's request.\n",
    "        \n",
    "        Args:\n",
    "            tool_call: Dictionary with 'name' and 'arguments' for the tool\n",
    "            \n",
    "        Returns:\n",
    "            The result from executing the tool\n",
    "        \"\"\"\n",
    "        tool_name = tool_call.get(\"name\")\n",
    "        arguments = tool_call.get(\"arguments\", {})\n",
    "        \n",
    "        if tool_name not in self.tools:\n",
    "            return f\"Error: Tool '{tool_name}' not found\"\n",
    "            \n",
    "        tool = self.tools[tool_name]\n",
    "        \n",
    "        # Validate and convert argument types using the tool's specification\n",
    "        try:\n",
    "            if hasattr(tool, \"specification\"):\n",
    "                tool_spec = json.loads(tool.specification)\n",
    "                validated_args = self.convert_argument_types(\n",
    "                    {\"arguments\": arguments}, \n",
    "                    tool_spec\n",
    "                )[\"arguments\"]\n",
    "                arguments = validated_args\n",
    "        except (json.JSONDecodeError, AttributeError, KeyError):\n",
    "            # Continue with original arguments if validation fails\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            # Handle execution based on the tool interface\n",
    "            # First try the execute method for tools created with the @tool decorator\n",
    "            if hasattr(tool, \"execute\"):\n",
    "                return tool.execute(**arguments)\n",
    "            # Then try the function attribute which is used by the @tool decorator\n",
    "            elif hasattr(tool, \"function\"):\n",
    "                return tool.function(**arguments)\n",
    "            # Fall back to run method\n",
    "            elif hasattr(tool, \"run\"):\n",
    "                return tool.run(**arguments)\n",
    "            # Last resort: call the tool directly if it's callable\n",
    "            elif callable(tool):\n",
    "                return tool(**arguments)\n",
    "            else:\n",
    "                return f\"Error: Tool '{tool_name}' is not callable\"\n",
    "        except Exception as e:\n",
    "            return f\"Error executing {tool_name}: {str(e)}\"\n",
    "    \n",
    "    def convert_argument_types(self, tool_call: Dict[str, Any], tool_spec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert arguments to their expected types based on tool specification.\n",
    "        \n",
    "        Args:\n",
    "            tool_call: Dictionary containing arguments to convert\n",
    "            tool_spec: Tool specification with expected types\n",
    "            \n",
    "        Returns:\n",
    "            Updated tool call with properly typed arguments\n",
    "        \"\"\"\n",
    "        if \"parameters\" not in tool_spec or \"properties\" not in tool_spec[\"parameters\"]:\n",
    "            return tool_call\n",
    "            \n",
    "        properties = tool_spec[\"parameters\"][\"properties\"]\n",
    "        \n",
    "        # Standard type converters\n",
    "        type_mapping = {\n",
    "            \"int\": int,\n",
    "            \"str\": str,\n",
    "            \"bool\": bool,\n",
    "            \"float\": float,\n",
    "            \"integer\": int,\n",
    "            \"string\": str,\n",
    "            \"boolean\": bool,\n",
    "            \"number\": float\n",
    "        }\n",
    "        \n",
    "        for arg_name, arg_value in tool_call[\"arguments\"].items():\n",
    "            if arg_name in properties and \"type\" in properties[arg_name]:\n",
    "                expected_type = properties[arg_name][\"type\"]\n",
    "                \n",
    "                if expected_type in type_mapping:\n",
    "                    converter = type_mapping[expected_type]\n",
    "                    try:\n",
    "                        # Only convert if types don't match\n",
    "                        if not isinstance(arg_value, converter):\n",
    "                            tool_call[\"arguments\"][arg_name] = converter(arg_value)\n",
    "                    except (ValueError, TypeError):\n",
    "                        # Keep original value if conversion fails\n",
    "                        pass\n",
    "                        \n",
    "        return tool_call\n",
    "    \n",
    "    def run(self, user_input: str) -> str:\n",
    "        # Add user input to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        final_response = \"\"\n",
    "        iterations = 0\n",
    "        current_prompt = user_input\n",
    "\n",
    "        while iterations < self.max_iterations:\n",
    "            iterations += 1\n",
    "\n",
    "            # Build the messages list for OpenAI API\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt + \"\\n\\n\" + self.format_tools_for_prompt()}\n",
    "            ]\n",
    "            # Add conversation history\n",
    "            for message in self.conversation_history:\n",
    "                messages.append({\"role\": message[\"role\"], \"content\": message[\"content\"]})\n",
    "\n",
    "            # If this isn't the first iteration, add the current context\n",
    "            if iterations > 1:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": current_prompt})\n",
    "\n",
    "            # Get response from language model\n",
    "            response = self.model(model=\"gpt-4o\", messages=messages)\n",
    "            model_response = response.choices[0].message.content\n",
    "\n",
    "            # Extract tool calls from response\n",
    "            tool_calls = self.extract_tool_calls(model_response)\n",
    "            \n",
    "            # If no tool calls or reached max iterations, return the response\n",
    "            if not tool_calls or iterations == self.max_iterations:\n",
    "                final_response = model_response\n",
    "                break\n",
    "                \n",
    "            # Execute tools and collect results\n",
    "            results = []\n",
    "            for tool_call in tool_calls:\n",
    "                result = self.execute_tool(tool_call)\n",
    "                results.append({\n",
    "                    \"tool\": tool_call.get(\"name\"),\n",
    "                    \"arguments\": tool_call.get(\"arguments\"),\n",
    "                    \"result\": result\n",
    "                })\n",
    "                \n",
    "            # Format results for next iteration\n",
    "            current_prompt = \"Tool results:\\n\"\n",
    "            for res in results:\n",
    "                result_str = str(res[\"result\"])\n",
    "                if isinstance(res[\"result\"], (dict, list)):\n",
    "                    try:\n",
    "                        result_str = json.dumps(res[\"result\"], indent=2)\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "                current_prompt += f\"- {res['tool']}{json.dumps(res['arguments'])}: {result_str}\\n\"\n",
    "        \n",
    "        # Add final response to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": final_response\n",
    "        })\n",
    "        \n",
    "        return final_response\n",
    "        \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Clear the conversation history.\"\"\"\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ToolCallingAgent(llm=client.chat.completions.create, system_prompt=system_prompt, tools=[calculate_area_of_rectangle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'calculate_area_of_rectangle', 'arguments': {'length': 10.0, 'width': 20.0}}]\n",
      "[{'name': 'calculate_area_of_rectangle', 'arguments': {'length': 10.0, 'width': 20.0}}]\n",
      "[{'name': 'calculate_area_of_rectangle', 'arguments': {'length': 10.0, 'width': 20.0}}]\n",
      "[{'name': 'calculate_area_of_rectangle', 'arguments': {'length': 10.0, 'width': 20.0}}]\n",
      "[{'name': 'calculate_area_of_rectangle', 'arguments': {'length': 10.0, 'width': 20.0}}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<tool_call>\\n{\"name\": \"calculate_area_of_rectangle\", \"arguments\": {\"length\": 10.0, \"width\": 20.0}}\\n</tool_call>'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is the area of a rectangle with a length of 10 and a width of 20?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create a diagram that explains how the ToolCallingAgent works and then explain the key components.\n",
    "\n",
    "\n",
    "## ToolCallingAgent: How It Works\n",
    "\n",
    "The diagram illustrates how the ToolCallingAgent orchestrates interactions between a language model and tool functions. Here's a breakdown of the key components and process flow:\n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **Language Model (LLM)**\n",
    "   - The AI system that generates responses and decides when to use tools\n",
    "   - Receives formatted prompts and returns responses that may include tool calls\n",
    "\n",
    "2. **Tools Registry**\n",
    "   - Collection of functions decorated with `@tool`\n",
    "   - Each tool has a name, specification (JSON schema), and implementation\n",
    "   - Tools are registered with the agent during initialization\n",
    "\n",
    "3. **Agent Core Logic**\n",
    "   - Manages conversation history and context\n",
    "   - Processes tool calls extracted from LLM responses\n",
    "   - Validates and converts argument types for tool execution\n",
    "   - Formats tool results for the LLM\n",
    "\n",
    "### Execution Flow\n",
    "\n",
    "1. **User Input Processing**\n",
    "   - User query is added to conversation history\n",
    "   - The agent prepares to process the request\n",
    "\n",
    "2. **LLM Prompt Creation**\n",
    "   - Tools are formatted into a structured format the LLM can understand\n",
    "   - System prompt, tools specifications, and conversation history are combined\n",
    "   - The complete prompt is sent to the language model\n",
    "\n",
    "3. **Tool Call Extraction & Execution**\n",
    "   - Agent parses the LLM response for `<tool_call>` tags\n",
    "   - For each tool call, the agent:\n",
    "     - Identifies the requested tool\n",
    "     - Validates and converts argument types\n",
    "     - Executes the tool and captures results\n",
    "\n",
    "4. **Result Processing**\n",
    "   - Tool results are formatted and sent back to the LLM\n",
    "   - This allows the LLM to incorporate tool outputs into its reasoning\n",
    "\n",
    "5. **Iterative Reasoning**\n",
    "   - Steps 2-4 repeat if more tool calls are needed\n",
    "   - Limited to maximum iterations to prevent infinite loops\n",
    "\n",
    "6. **Final Response**\n",
    "   - When no more tool calls are needed, the final response is returned to the user\n",
    "   - The complete interaction is saved in conversation history\n",
    "\n",
    "This architecture enables seamless integration between language models and functional tools, allowing the LLM to access external capabilities when needed to fulfill user requests.\n",
    "\n",
    "![Tool Calling Agent](../../../images/tool_calling_agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Class Implementation\n",
    "\n",
    "Now that we have gone over the basics of how the ToolCallingAgent works, let's implement a custom class that uses the ToolCallingAgent to solve a problem or in other projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-agent-patterns-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
